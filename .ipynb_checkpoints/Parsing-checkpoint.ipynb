{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503ac379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164afb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Egemen.kz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1d5d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re as re\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6387eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the URL for articles in Egemen.kz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "536e5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(base_url,search_url):\n",
    "    response = requests.get(search_url) # It is the default\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Парсинг HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    divs = soup.find_all('div', class_='clearfix news-t flexBlock')\n",
    "\n",
    "    # Extract URLs from <a> tags inside those divs\n",
    "    urls = [div.a['href'] for div in divs if div.a]\n",
    "    full_urls = [base_url.rstrip('/') + div.a['href'] for div in divs if div.a]\n",
    "\n",
    "    return full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7af9ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the function specifically for the parsing Egemen.kz web-site content and saved as a text file\n",
    "# Only works for one URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49b89a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_contents(article_url, storage_dir):\n",
    "    \n",
    "    try:\n",
    "        # Загрузка HTML страницы\n",
    "        response = requests.get(article_url)\n",
    "        \n",
    "        # Check for 404 errors or other HTTP status codes\n",
    "        if response.status_code != 200:\n",
    "            raise requests.HTTPError(f\"HTTP Error: {response.status_code} for {article_url}\")\n",
    "        \n",
    "        html_content = response.text\n",
    "        \n",
    "        # Парсинг HTML\n",
    "        soup = BS(html_content, \"html.parser\")\n",
    "        \n",
    "        # Title extract\n",
    "        try:\n",
    "            title = soup.find('h1')\n",
    "            title = title.get_text(strip=True)\n",
    "        except Exception: \n",
    "            title = \"Title not found\"\n",
    "        \n",
    "        # Author extract\n",
    "        try:\n",
    "            author = soup.find('div', class_='name-auth').text.strip()\n",
    "        except Exception: \n",
    "            author = \"Author not found\"\n",
    "        \n",
    "        # Date extract\n",
    "        try:\n",
    "            date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "            if date_tag and date_tag.has_attr('content'):\n",
    "                date_published = date_tag['content']\n",
    "                date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "            else:\n",
    "                date_published = \"Date not found\"\n",
    "        except Exception: \n",
    "            date_published = \"Date not found\"\n",
    "        \n",
    "        # Content extract\n",
    "        try:\n",
    "            article_body = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "            article_text = article_body.get_text(separator=\"\\n\", strip=True) if article_body else \"Content not found\"\n",
    "        except Exception: \n",
    "            article_text = \"Content not found\"\n",
    "        \n",
    "        # Сохранение текста в файл\n",
    "        valid_title = re.sub(r'[\\\\/:\"*?<>|]+', '', title)  # Remove invalid characters\n",
    "        filename = os.path.join(storage_dir, f\"{valid_title}.txt\")\n",
    "        \n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"{title}\\n\\n\")\n",
    "            file.write(article_text)\n",
    "        \n",
    "        print(\"Статья успешно сохранена!\")\n",
    "        \n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"Date Published\": date_published,\n",
    "            \"Author\": author,\n",
    "            \"URL\": article_url,\n",
    "            \"Status\": \"Success\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2bb5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_details(article_url):\n",
    "    \"\"\"Extract title, date published, author, and URL.\"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_published = date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extract author\n",
    "    author_tag = soup.find('div', class_='name-auth')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"No Author\"\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e839c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Key word input\n",
    "    key_word_input = input(\"Введите ключевое слово: \\n\")\n",
    "    \n",
    "    # Creating url with key-word\n",
    "    base_url_search = \"https://egemen.kz\"                         #base url can be changed by other URL\n",
    "    search_url = base_url_search + \"/search?q=\" \n",
    "    words = key_word_input.split()\n",
    "    string = \"+\".join(words)\n",
    "    key_word_url = search_url + string\n",
    "\n",
    "    print(f\"Ссылка по вашему запросу: \\n{key_word_url}\")\n",
    "    \n",
    "    # Creating directory with the name of the input \n",
    "    storage_dir = os.path.join(os.getcwd(), key_word_input)\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(key_word_url) \n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parsing HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Finding articles number, included in web-site\n",
    "    article_founded = soup.find('small').text\n",
    "\n",
    "    print(article_founded)\n",
    "    \n",
    "    # Conversion to int from list\n",
    "    num_article = re.findall(r'\\d+', article_founded)\n",
    "    num = int(num_article[0]) \n",
    "    \n",
    "    # Each web-page in site only contains 5 articels, it can be also changed \n",
    "    articles_per_page = 5\n",
    "    pages = math.ceil(num / articles_per_page) \n",
    "\n",
    "    print(f\"Чило страниц:{pages}\\n\")\n",
    "    \n",
    "    urls_list = []  # Initialize the list\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        full_url = key_word_url + \"&page=\" + str(page)\n",
    "        print(f\"Processing page: {page} {full_url}\\n\")\n",
    "    \n",
    "        # Call the function and get the extracted URLs\n",
    "        article_urls = extract_urls(base_url_search, full_url)\n",
    "    \n",
    "        for article_url in article_urls: \n",
    "            print(f\"{article_url}\\n\")    \n",
    "    \n",
    "        # Append URLs to the list\n",
    "        urls_list.extend(article_urls) \n",
    "    \n",
    "        # Skip if no URLs are found\n",
    "        if not article_urls:  \n",
    "            print(f\"Skipping page {page} due to no articles found.\\n\")\n",
    "        continue\n",
    "\n",
    "    # The final list of URLs\n",
    "    print(\"\\nAll Extracted URLs:\\n\")\n",
    "\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "    \n",
    "    print(f\"\\nCount:{len(urls_list)}\\n\")\n",
    "    print(\"Starting saving all the extracted articles to the text\")\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "        article_contents(url,storage_dir)\n",
    "        \n",
    "    # CSV file creation\n",
    "    csv_file = os.path.join(storage_dir, f\"{key_word_input}.csv\")\n",
    "    \n",
    "    # CSV headers\n",
    "    headers = [\"Title\", \"Date Published\", \"Author\", \"URL\"]\n",
    "    \n",
    "\n",
    "        \n",
    "    # Writing to CSV\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for url in urls_list:\n",
    "            print(f\"Processing article: {url}\")\n",
    "            article_details = extract_article_details(url)\n",
    "            writer.writerow(article_details)\n",
    "    print(\"CSV файл успешно сохранен !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87039897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите ключевое слово: \n",
      "Рынок\n",
      "Ссылка по вашему запросу: \n",
      "https://egemen.kz/search?q=Рынок\n",
      "7 материал табылды\n",
      "Чило страниц:2\n",
      "\n",
      "Processing page: 1 https://egemen.kz/search?q=Рынок&page=1\n",
      "\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "\n",
      "Processing page: 2 https://egemen.kz/search?q=Рынок&page=2\n",
      "\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "\n",
      "\n",
      "All Extracted URLs:\n",
      "\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "\n",
      "Count:7\n",
      "\n",
      "Starting saving all the extracted articles to the text\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "Статья успешно сохранена!\n",
      "Processing article: https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "Processing article: https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "Processing article: https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "Processing article: https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "Processing article: https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "Processing article: https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "Processing article: https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "CSV файл успешно сохранен !\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96804bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите ключевое слово: \n",
      "Рынок\n",
      "Ссылка по вашему запросу: \n",
      "https://egemen.kz/search?q=Рынок\n",
      "7 материал табылды\n",
      "Чило страниц:2\n",
      "\n",
      "Processing page: 1 https://egemen.kz/search?q=Рынок&page=1\n",
      "\n",
      "['https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty', 'https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter', 'https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek', 'https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty', 'https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik']\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "\n",
      "Processing page: 2 https://egemen.kz/search?q=Рынок&page=2\n",
      "\n",
      "['https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly', 'https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru']\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "\n",
      "\n",
      "All Extracted URLs:\n",
      "\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "\n",
      "Count:7\n",
      "\n",
      "Starting saving all the extracted articles to text\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "Статья успешно сохранена!\n",
      "CSV файл успешно сохранен в /home/alikhan/Desktop/Data/Parsing/Рынок/Рынок.csv!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re as re\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "def article_contents(article_url, storage_dir):\n",
    "    try:\n",
    "        # Загрузка HTML страницы\n",
    "        response = requests.get(article_url)\n",
    "        \n",
    "        # Check for 404 errors or other HTTP status codes\n",
    "        if response.status_code != 200:\n",
    "            raise requests.HTTPError(f\"HTTP Error: {response.status_code} for {article_url}\")\n",
    "        \n",
    "        html_content = response.text\n",
    "        \n",
    "        # Парсинг HTML\n",
    "        soup = BS(html_content, \"html.parser\")\n",
    "        \n",
    "        # Title extract\n",
    "        try:\n",
    "            title = soup.find('h1')\n",
    "            title = title.get_text(strip=True)\n",
    "        except Exception: \n",
    "            title = \"Title not found\"\n",
    "        \n",
    "        # Author extract\n",
    "        try:\n",
    "            author = soup.find('div', class_='name-auth').text.strip()\n",
    "        except Exception: \n",
    "            author = \"Author not found\"\n",
    "        \n",
    "        # Date extract\n",
    "        try:\n",
    "            date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "            if date_tag and date_tag.has_attr('content'):\n",
    "                date_published = date_tag['content']\n",
    "                date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "            else:\n",
    "                date_published = \"Date not found\"\n",
    "        except Exception: \n",
    "            date_published = \"Date not found\"\n",
    "        \n",
    "        # Content extract\n",
    "        try:\n",
    "            article_body = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "            article_text = article_body.get_text(separator=\"\\n\", strip=True) if article_body else \"Content not found\"\n",
    "        except Exception: \n",
    "            article_text = \"Content not found\"\n",
    "        \n",
    "        # Сохранение текста в файл\n",
    "        valid_title = re.sub(r'[\\\\/:\"*?<>|]+', '', title)  # Remove invalid characters\n",
    "        filename = os.path.join(storage_dir, f\"{valid_title}.txt\")\n",
    "        \n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"{title}\\n\\n\")\n",
    "            file.write(article_text)\n",
    "        \n",
    "        print(\"Статья успешно сохранена!\")\n",
    "        \n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"Date Published\": date_published,\n",
    "            \"Author\": author,\n",
    "            \"URL\": article_url,\n",
    "            \"Source\": article_url.split('/')[2],\n",
    "            \"Status\": \"Success\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def extract_article_details(article_url):\n",
    "    \"\"\"Extract title, date published, author, and URL.\"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_published = date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extract author\n",
    "    author_tag = soup.find('div', class_='name-auth')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"No Author\"\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url,\n",
    "        \"Source\": article_url.split('/')[2]\n",
    "    }\n",
    "\n",
    "def save_to_csv(data_list, csv_file, headers):\n",
    "    \"\"\"Saves a list of article details to a CSV file.\"\"\"\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_list)\n",
    "    print(f\"CSV файл успешно сохранен в {csv_file}!\")\n",
    "\n",
    "def main():\n",
    "    # Key word input\n",
    "    key_word_input = input(\"Введите ключевое слово: \\n\")\n",
    "    \n",
    "    # Creating URL with key-word\n",
    "    base_url_search = \"https://egemen.kz\"                         # Base URL can be changed by other URL\n",
    "    search_url = base_url_search + \"/search?q=\" \n",
    "    words = key_word_input.split()\n",
    "    string = \"+\".join(words)\n",
    "    key_word_url = search_url + string\n",
    "\n",
    "    print(f\"Ссылка по вашему запросу: \\n{key_word_url}\")\n",
    "    \n",
    "    # Creating directory with the name of the input \n",
    "    storage_dir = os.path.join(os.getcwd(), key_word_input)\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(key_word_url) \n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parsing HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Finding articles number included in the website\n",
    "    article_founded = soup.find('small').text\n",
    "\n",
    "    print(article_founded)\n",
    "    \n",
    "    # Conversion to int from list\n",
    "    num_article = re.findall(r'\\d+', article_founded)\n",
    "    num = int(num_article[0]) \n",
    "    \n",
    "    # Each web-page in site only contains 5 articles, it can also be changed \n",
    "    articles_per_page = 5\n",
    "    pages = math.ceil(num / articles_per_page) \n",
    "\n",
    "    print(f\"Чило страниц:{pages}\\n\")\n",
    "    \n",
    "    urls_list = []  # Initialize the list\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        full_url = key_word_url + \"&page=\" + str(page)\n",
    "        print(f\"Processing page: {page} {full_url}\\n\")\n",
    "    \n",
    "        # Call the function and get the extracted URLs\n",
    "        article_urls = extract_urls(base_url_search, full_url)\n",
    "    \n",
    "        for article_url in article_urls: \n",
    "            print(f\"{article_url}\\n\")    \n",
    "    \n",
    "        # Append URLs to the list\n",
    "        urls_list.extend(article_urls) \n",
    "    \n",
    "        # Skip if no URLs are found\n",
    "        if not article_urls:  \n",
    "            print(f\"Skipping page {page} due to no articles found.\\n\")\n",
    "        continue\n",
    "\n",
    "    # The final list of URLs\n",
    "    print(\"\\nAll Extracted URLs:\\n\")\n",
    "\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "    \n",
    "    print(f\"\\nCount:{len(urls_list)}\\n\")\n",
    "    print(\"Starting saving all the extracted articles to text\")\n",
    "\n",
    "    articles_data = []  # To store article details for CSV\n",
    "\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "        article_detail = article_contents(url, storage_dir)\n",
    "        articles_data.append(article_detail)\n",
    "\n",
    "    # CSV file creation\n",
    "    csv_file = os.path.join(storage_dir, f\"{key_word_input}.csv\")\n",
    "    \n",
    "    # CSV headers\n",
    "    headers = [\"Title\", \"Date Published\", \"Author\", \"URL\", \"Source\", \"Status\"]\n",
    "    \n",
    "    # Save to CSV\n",
    "    save_to_csv(articles_data, csv_file, headers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Tengri-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49307483",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://tengrinews.kz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32fc5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(base_url,search_url):\n",
    "    response = requests.get(search_url) # It is the default\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Парсинг HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    divs = soup.find_all('div', class_='content_main_item')\n",
    "\n",
    "    # Extract URLs from <a> tags inside those divs\n",
    "    urls = [div.a['href'] for div in divs if div.a]\n",
    "    full_urls = [base_url.rstrip('/') + div.a['href'] for div in divs if div.a]\n",
    "\n",
    "    return full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d66f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input key word:Донор\n",
      "Processing page:1\n",
      "\n",
      "https://tengrinews.kz/mixnews/mark-tsukerberg-meta-otkajetsya-faktchekinga-facebook-559053/\n",
      "\n",
      "https://tengrinews.kz/usa/bayden-obratilsya-strane-svyazi-pobedoy-trampa-vyiborah-553322/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/sledam-pavla-durova-kazahstantsyi-stanovyatsya-donorami-543721/\n",
      "\n",
      "https://tengrinews.kz/curious/v-reanimatsii-povtoryal-imya-neznakomogo-cheloveka-istoriya-543236/\n",
      "\n",
      "https://tengrinews.kz/world_news/vtoroy-patsient-vyilechilsya-ot-vich-v-germanii-541917/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/si-tszinpin-napisal-statyu-obschem-stremlenii-kitaya-540060/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/stat-geroem-prosto-sdat-stvolovyie-kletki-spasti-bolnogo-533819/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/balanyin-fotosuretne-ruyina-karap-tandauga-boladyi-elmzde-531199/\n",
      "\n",
      "https://tengrinews.kz/healthy/mogut-donoryi-krovi-poluchat-zakonnyie-otgulyi-kazahstane-507042/\n",
      "\n",
      "https://tengrinews.kz/article/aura-kazino-semya-neobyichnyie-podarki-intervyu-akimom-2084/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/53-letniy-jitel-shyimkenta-pojertvoval-pochku-svoey-docheri-499241/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/v-aktobe-mujchina-spas-jizn-supruge-otdav-ey-svoyu-pochku-493963/\n",
      "\n",
      "https://tengrinews.kz/profitably/skidki-zdes-gde-deshevo-razvlechsya-i-pokushatvalmatyi-488629/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/unikalnuyu-metodiku-lechenii-onkologii-vpervyie-primenili-485449/\n",
      "\n",
      "https://tengrinews.kz/curious/pochka-lyubov-melodrama-donor-vyiydet-prokat-17-noyabrya-483356/\n",
      "\n",
      "https://tengrinews.kz/medicine/transplantatsiya-kostnogo-mozga-stala-bezopasnoy-dostupnoy-474024/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/semya-zabolevshego-rakom-krovi-6-letnego-almatintsa-470898/\n",
      "\n",
      "https://tengrinews.kz/science/virus-obnarujili-serdtse-svini-peresajennom-patsientu-ssha-468119/\n",
      "\n",
      "https://tengrinews.kz/medicine/veduschie-spetsialistyi-acbadem-provodyat-transplantatsiyu-466141/\n",
      "\n",
      "https://tengrinews.kz/medicine/hirurgi-ssha-vpervyie-peresadili-cheloveku-srazu-dve-svinyie-459805/\n",
      "\n",
      "Processing page:2\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/dargerler-onyin-ush-ay-gana-uakyityi-kaldyi-ded-456470/\n",
      "\n",
      "https://tengrinews.kz/world_news/norvegiya-prizvala-donorov-vsemirnogo-banka-afganistane-455947/\n",
      "\n",
      "https://tengrinews.kz/world_news/chto-proizoshlo-v-mire-nochyu-2-dekabrya-455484/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/270-umirayut-ejegodno-kazahstane-ne-dojdavshis-organa-453017/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/kazakstanda-jartyi-jyilda-147-adam-donor-tappay-kaytyis-444527/\n",
      "\n",
      "https://tengrinews.kz/other/ukraintsyi-prodayut-pochki-bogatyim-inostrantsam-smi-439561/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/lish-tiho-proiznes-mama-ya-tak-ustal-ot-boli-437941/\n",
      "\n",
      "https://tengrinews.kz/sng/transplantatsiyu-organov-ne-ot-rodstvennikov-razreshat-433372/\n",
      "\n",
      "https://tengrinews.kz/science/znamenitaya-chernaya-dyira-okazalas-21-massivnee-solntsa-429458/\n",
      "\n",
      "https://tengrinews.kz/medicine/covid-19-juktyirgandardyi-kan-plazmasyin-salyip-emdeu-407862/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/pochemu-kazahstantsyi-sdayut-krov-bolnyih-koronavirusom-407142/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/nadyirov-turgyindardyi-koronaviruska-karsyiantidenege-kan-406202/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/ya-bolel-koronavirusom-oni-pervyie-donoryi-plazmyi-402130/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/birtanov-donorstve-vrachi-znayut-soglasen-patsient-net-393597/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/nagyiz-azamattyin-s-tokaev-kmderge-jane-ushn-algyis-aytkan-393443/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/transplantatsiya-organov-spornyie-momentyi-kodekse-389764/\n",
      "\n",
      "https://tengrinews.kz/medicine/okolo-100-kazahstantsev-umirayut-ejegodno-dojdavshis-384712/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/kazahstane-skandala-hotyat-zapretit-peresadku-organov-381817/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/chto-delayut-s-krovyu-donorov-v-nur-sultane-378275/\n",
      "\n",
      "https://tengrinews.kz/story/serdtse-indiyskogo-malchika-podarilo-devochke-kokshetau-376881/\n",
      "\n",
      "Processing page:3\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/v-vzglyade-uvidela-syina-odin-askar-smerti-spas-4-jizni-375778/\n",
      "\n",
      "https://tengrinews.kz/medicine/ote-belsend-ar-omrge-kushtar-maman-donor-boludyin-paydasyin-371370/\n",
      "\n",
      "https://tengrinews.kz/story/katerl-skt-jenp-shyikkan-almatyilyik-jas-jazushyi-kyiz-367291/\n",
      "\n",
      "https://tengrinews.kz/football/bolelschik-fk-aktobe-stal-invalidom-matcha-professionalami-366654/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/eto-galiya-bulekbaeva-ona-umerla-ne-dojdavshis-donora-pochki-364953/\n",
      "\n",
      "https://tengrinews.kz/medicine/mujchina-izlechilsya-ot-vich-v-velikobritanii-364496/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/juregn-auyistyirgan-astanalyik-otadan-keyn-bolgan-tyilsyim-360191/\n",
      "\n",
      "https://tengrinews.kz/medicine/ush-buyreg-bar-adamdar-donor-bola-ala-ma-358622/\n",
      "\n",
      "https://tengrinews.kz/story/lyudi-osoznanno-otdayut-organyi-blizkim-vrach-rasskazal-356528/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/etapirovannyiy-astanu-donor-bolnoy-rakom-jenschinyi-354956/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/chto-teper-mnoy-budet-edinstvennyiy-donor-bolnoy-kostanayki-353613/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/peresadka-stvolovyih-kletok-kazahstantsyi-mogut-pomoch-352394/\n",
      "\n",
      "https://tengrinews.kz/story/buyregn-muktaj-janga-bergen-jgt-mahabbatyin-internetten-347770/\n",
      "\n",
      "https://tengrinews.kz/world_news/spasshiy-millionyi-detey-donor-ushel-na-pokoy-344407/\n",
      "\n",
      "https://tengrinews.kz/crime/smert-donora-v-almatyi-detali-dela-o-kuple-prodaje-pochki-337050/\n",
      "\n",
      "https://tengrinews.kz/crime/smert-donora-v-almatyi-pokupatelyu-pochki-vyinesli-prigovor-337051/\n",
      "\n",
      "https://tengrinews.kz/medicine/chto-nujno-znat-chtobyi-stat-donorom-329542/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/nazarbaev-vyiskazalsya-genplane-vyirubke-derevev-almatyi-326950/\n",
      "\n",
      "https://tengrinews.kz/medicine/yuriy-pya-kazahstan-ne-gotov-k-detskomu-donorstvu-324454/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/korruptsionnaya-sostavlyayuschaya-nevozmojna-trupnom-321336/\n",
      "\n",
      "Processing page:4\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/kolichestvo-transplantatsiy-rk-sokratilos-iz-za-publikatsiy-321327/\n",
      "\n",
      "https://tengrinews.kz/medicine/italyanskiy-hirurg-otkazalsya-peresajivat-golovu-rossiyaninu-320629/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/almatinets-sdal-polutonnyi-krovi-poluchil-zvanie-320294/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/ministr-o-smerti-donora-v-almatyi-situatsiya-neordinarnaya-314705/\n",
      "\n",
      "https://tengrinews.kz/crime/politsiya-rassleduet-smert-donora-pochki-v-bolnitse-almatyi-314698/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/volonteryi-zapodozrili-almatinku-nedobrosovestnom-312929/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/70-protsentov-kazahstantsev-otkazyivayutsya-posmertnogo-305702/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/bolee-100-litrov-krovi-sdal-kazahstanskiy-donor-305520/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/izbejat-moshennichestva-okazanii-pomoschi-bolnyim-detyam-301732/\n",
      "\n",
      "https://tengrinews.kz/usa/prestijnyiy-amerikanskiy-vuz-vyistupil-protiv-301127/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/dvuhletney-tahmine-abyishevoy-peresadili-kostnyiy-mozg-299141/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/kitay-vyinujden-postavlyat-svoih-rabochih-kazahstan-ekspert-288408/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/pomenyat-operatora-svyazi-otmenyi-mobilnogo-rabstva-286640/\n",
      "\n",
      "https://tengrinews.kz/medicine/v-kazahstane-vpervyie-nezakonno-izyyali-chelovecheskiy-organ-283261/\n",
      "\n",
      "https://tengrinews.kz/story/kazahstanskiy-pevets-otvetil-post-razlichii-almatyi-astanyi-280092/\n",
      "\n",
      "https://tengrinews.kz/europe/detyam-donorov-spermyi-razreshili-uznavat-imya-ottsa-lyubom-269204/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/2-tyisyach-kazahstantsev-nujdayutsya-transplantatsii-organov-259166/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/posmertnoe-donorstvo-organov-kazahstane-podderjal-verhovnyiy-244198/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/donorskaya-yaytsekletka-kazahstane-okazalas-deshevle-243477/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/uvelichit-vdvoe-kolichestvo-donorov-krovi-neobhodimo-236178/\n",
      "\n",
      "Processing page:5\n",
      "\n",
      "https://tengrinews.kz/medicine/kazahstane-2013-godu-zaplanirovano-60-peresadok-229706/\n",
      "\n",
      "https://tengrinews.kz/medicine/v-yujnom-kazahstane-vpervyie-peresadili-donorskuyu-pochku-229620/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/doskaliev-prizyivaet-kazahstantsev-prodavat-svoi-organyi-224579/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/ahan-sataev-serik-sapiev-nazvanyi-natsionalnyimi-patriotami-224464/\n",
      "\n",
      "https://tengrinews.kz/story/gollandets-za-8-let-stal-ottsom-82-detey-219026/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/prezident-kazahstana-predlojil-ideyu-obschestva-vseobschego-217150/\n",
      "\n",
      "https://tengrinews.kz/private_finance/astana-obyedaet-ostalnyih-kazahstantsev-217134/\n",
      "\n",
      "https://tengrinews.kz/medicine/v-kazahstane-defitsit-donorov-spermyi-209097/\n",
      "\n",
      "https://tengrinews.kz/allsports/uchastie-astanyi-na-dakare-2013-okazalos-pod-voprosom-206568/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/kajdyiy-vosmoy-kazahstanets-jivet-v-almatyi-201600/\n",
      "\n",
      "https://tengrinews.kz/kazakhstan_news/pervaya-operatsiya-peresadke-serdtsa-kazahstane-199630/\n",
      "\n",
      "https://tengrinews.kz/medicine/v-turtsii-vpervyie-sdelali-operatsiyu-po-peresadke-matki-198123/\n",
      "\n",
      "https://tengrinews.kz/medicine/shvedyi-vpervyie-v-mire-peresadili-sinteticheskuyu-traheyu-192223/\n",
      "\n",
      "https://tengrinews.kz/story/55-letnyaya-britanka-otdast-svoyu-matku-docheri-190409/\n",
      "\n",
      "https://tengrinews.kz/crime/almatyi-skryivshegosya-eks-glavu-spid-tsentra-zaveli-187893/\n",
      "\n",
      "https://tengrinews.kz/news/zarajennyih-gepatitom-s-detey-nachnut-lechit-roferonom-47667/\n",
      "\n",
      "https://tengrinews.kz/unsort/krizis-vyinudil-britantsev-prodavat-svoi-pochki-26163/\n",
      "\n",
      "https://tengrinews.kz/unsort/britanii-zapretyat-operatsii-peresadke-organov-inostrantsam-21131/\n",
      "\n",
      "https://tengrinews.kz/accidents/auktsion-chelovecheskih-organov-17938/\n",
      "\n",
      "https://tengrinews.kz/news/kazahstanskie-pravozaschitniki-trebuyut-izmenit-zakon-6280/\n",
      "\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "key_word_input = input(\"Input key word:\")\n",
    "base_url = \"https://tengrinews.kz\"\n",
    "search_url = base_url + \"/search/?text=\"\n",
    "words = key_word_input.split()\n",
    "string = \"+\".join(words)\n",
    "key_word_url = search_url + string\n",
    "\n",
    "pages = 5 # Counting manually how much page usually indicates in the front page. \n",
    "articles_count = 0\n",
    "\n",
    "for page in range(1, pages + 1):  # Итерируемся от 1 до 5 включительно\n",
    "    full_url = base_url + \"/search/page/\" + str(page) + \"/?field=all&text=\" + string +\"&sort=date\"\n",
    "    print(f\"Processing page:{page}\\n\")\n",
    "    # Call the function and get the extracted URLs\n",
    "    article_urls = extract_urls(base_url, full_url)\n",
    "    \n",
    "    for article_url in article_urls: \n",
    "         print(f\"{article_url}\\n\")    \n",
    "    \n",
    "     # Append URLs to the list\n",
    "    urls_list.extend(article_urls) \n",
    "    \n",
    "    articles_count += len(article_urls)\n",
    "    \n",
    "        # Skip if no URLs are found\n",
    "    if not article_urls:  \n",
    "        print(f\"Skipping page {page} due to no articles found.\\n\")\n",
    "    continue\n",
    "print(articles_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d6532ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_details(article_url):\n",
    "    \"\"\"Extract title, date published, author, and URL.\"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_published = date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extract author\n",
    "    author_tag = soup.find('span', class_='content_main_meta_author_item_name')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"No Author\"\n",
    "    \n",
    "    information =  {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url,\n",
    "        \"Source\": article_url.split('/')[2]\n",
    "    }\n",
    "    \n",
    "    print(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723c159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28dfddba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'Шведы впервые в мире пересадили синтетическую трахею', 'Date Published': 'No Date', 'Author': 'Алдияр Косенов', 'URL': 'https://tengrinews.kz/medicine/shvedyi-vpervyie-v-mire-peresadili-sinteticheskuyu-traheyu-192223/', 'Source': 'tengrinews.kz'}\n"
     ]
    }
   ],
   "source": [
    "extract_article_details(\"https://tengrinews.kz/medicine/shvedyi-vpervyie-v-mire-peresadili-sinteticheskuyu-traheyu-192223/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tengrinews.kz/medicine/shvedyi-vpervyie-v-mire-peresadili-sinteticheskuyu-traheyu-192223/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18130af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_published = date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all('div', class_='content_main_meta_author_item_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "    urls = [div.a['href'] for div in divs if div.a]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
