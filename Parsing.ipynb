{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503ac379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d5d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re as re\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6387eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the URL for articles in Egemen.kz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536e5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(base_url,search_url):\n",
    "    response = requests.get(search_url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Парсинг HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    divs = soup.find_all('div', class_='clearfix news-t flexBlock')\n",
    "\n",
    "    # Extract URLs from <a> tags inside those divs\n",
    "    urls = [div.a['href'] for div in divs if div.a]\n",
    "    full_urls = [base_url.rstrip('/') + div.a['href'] for div in divs if div.a]\n",
    "\n",
    "    return full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af9ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the function specifically for the parsing Egemen.kz web-site content and saved as a text file\n",
    "# Only works for one URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b89a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_contents(article_url, storage_dir):\n",
    "    \n",
    "    try:\n",
    "        # Загрузка HTML страницы\n",
    "        response = requests.get(article_url)\n",
    "        \n",
    "        # Check for 404 errors or other HTTP status codes\n",
    "        if response.status_code != 200:\n",
    "            raise requests.HTTPError(f\"HTTP Error: {response.status_code} for {article_url}\")\n",
    "        \n",
    "        html_content = response.text\n",
    "        \n",
    "        # Парсинг HTML\n",
    "        soup = BS(html_content, \"html.parser\")\n",
    "        \n",
    "        # Title extract\n",
    "        try:\n",
    "            title = soup.find('h1')\n",
    "            title = title.get_text(strip=True)\n",
    "        except Exception: \n",
    "            title = \"Title not found\"\n",
    "        \n",
    "        # Author extract\n",
    "        try:\n",
    "            author = soup.find('div', class_='name-auth').text.strip()\n",
    "        except Exception: \n",
    "            author = \"Author not found\"\n",
    "        \n",
    "        # Date extract\n",
    "        try:\n",
    "            date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "            if date_tag and date_tag.has_attr('content'):\n",
    "                date_published = date_tag['content']\n",
    "                date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "            else:\n",
    "                date_published = \"Date not found\"\n",
    "        except Exception: \n",
    "            date_published = \"Date not found\"\n",
    "        \n",
    "        # Content extract\n",
    "        try:\n",
    "            article_body = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "            article_text = article_body.get_text(separator=\"\\n\", strip=True) if article_body else \"Content not found\"\n",
    "        except Exception: \n",
    "            article_text = \"Content not found\"\n",
    "        \n",
    "        # Сохранение текста в файл\n",
    "        valid_title = re.sub(r'[\\\\/:\"*?<>|]+', '', title)  # Remove invalid characters\n",
    "        filename = os.path.join(storage_dir, f\"{valid_title}.txt\")\n",
    "        \n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"{title}\\n\\n\")\n",
    "            file.write(article_text)\n",
    "        \n",
    "        print(\"Статья успешно сохранена!\")\n",
    "        \n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"Date Published\": date_published,\n",
    "            \"Author\": author,\n",
    "            \"URL\": article_url,\n",
    "            \"Status\": \"Success\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bb5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_details(article_url):\n",
    "    \"\"\"Extract title, date published, author, and URL.\"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_published = date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extract author\n",
    "    author_tag = soup.find('div', class_='name-auth')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"No Author\"\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e839c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Key word input\n",
    "    key_word_input = input(\"Введите ключевое слово: \\n\")\n",
    "    \n",
    "    # Creating url with key-word\n",
    "    base_url_search = \"https://egemen.kz\"                         #base url can be changed by other URL\n",
    "    search_url = base_url_search + \"/search?q=\" \n",
    "    words = key_word_input.split()\n",
    "    string = \"+\".join(words)\n",
    "    key_word_url = search_url + string\n",
    "\n",
    "    print(f\"Ссылка по вашему запросу: \\n{key_word_url}\")\n",
    "    \n",
    "    # Creating directory with the name of the input \n",
    "    storage_dir = os.path.join(os.getcwd(), key_word_input)\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(key_word_url) \n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parsing HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Finding articles number, included in web-site\n",
    "    article_founded = soup.find('small').text\n",
    "\n",
    "    print(article_founded)\n",
    "    \n",
    "    # Conversion to int from list\n",
    "    num_article = re.findall(r'\\d+', article_founded)\n",
    "    num = int(num_article[0]) \n",
    "    \n",
    "    # Each web-page in site only contains 5 articels, it can be also changed \n",
    "    articles_per_page = 5\n",
    "    pages = math.ceil(num / articles_per_page) \n",
    "\n",
    "    print(f\"Чило страниц:{pages}\\n\")\n",
    "    \n",
    "    urls_list = []  # Initialize the list\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        full_url = key_word_url + \"&page=\" + str(page)\n",
    "        print(f\"Processing page: {page} {full_url}\\n\")\n",
    "    \n",
    "        # Call the function and get the extracted URLs\n",
    "        article_urls = extract_urls(base_url_search, full_url)\n",
    "    \n",
    "        for article_url in article_urls: \n",
    "            print(f\"{article_url}\\n\")    \n",
    "    \n",
    "        # Append URLs to the list\n",
    "        urls_list.extend(article_urls) \n",
    "    \n",
    "        # Skip if no URLs are found\n",
    "        if not article_urls:  \n",
    "            print(f\"Skipping page {page} due to no articles found.\\n\")\n",
    "        continue\n",
    "\n",
    "    # The final list of URLs\n",
    "    print(\"\\nAll Extracted URLs:\\n\")\n",
    "\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "    \n",
    "    print(f\"\\nCount:{len(urls_list)}\\n\")\n",
    "    print(\"Starting saving all the extracted articles to the text\")\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "        article_contents(url,storage_dir)\n",
    "        \n",
    "    # CSV file creation\n",
    "    csv_file = os.path.join(storage_dir, f\"{key_word_input}.csv\")\n",
    "    \n",
    "    # CSV headers\n",
    "    headers = [\"Title\", \"Date Published\", \"Author\", \"URL\"]\n",
    "    \n",
    "\n",
    "        \n",
    "    # Writing to CSV\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for url in urls_list:\n",
    "            print(f\"Processing article: {url}\")\n",
    "            article_details = extract_article_details(url)\n",
    "            writer.writerow(article_details)\n",
    "    print(\"CSV файл успешно сохранен !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87039897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите ключевое слово: \n",
      "Рынок\n",
      "Ссылка по вашему запросу: \n",
      "https://egemen.kz/search?q=Рынок\n",
      "7 материал табылды\n",
      "Чило страниц:2\n",
      "\n",
      "Processing page: 1 https://egemen.kz/search?q=Рынок&page=1\n",
      "\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "\n",
      "Processing page: 2 https://egemen.kz/search?q=Рынок&page=2\n",
      "\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "\n",
      "\n",
      "All Extracted URLs:\n",
      "\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "\n",
      "Count:7\n",
      "\n",
      "Starting saving all the extracted articles to the text\n",
      "https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "Статья успешно сохранена!\n",
      "https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "Статья успешно сохранена!\n",
      "Processing article: https://egemen.kz/article/200045-gharyshtyq-eginshilik-qazaqstandyq-startap-egistic-rynokqa-shyqty\n",
      "Processing article: https://egemen.kz/article/95567-irandyq-vektor-dganha-rynoktar-dganha-mumkindikter\n",
      "Processing article: https://egemen.kz/article/29341-ishki-rynokta-turaqtylyq-kerek\n",
      "Processing article: https://egemen.kz/article/15983-qazaqstan-titany-alemdik-rynokqa-qadam-basty\n",
      "Processing article: https://egemen.kz/article/14439-alemdik-rynokqa-shyghudynh-basty-dgoly-–-basekege-qabilettilik\n",
      "Processing article: https://egemen.kz/article/10317-basekelestik-–-rynokty-bayytudynh-orkenietti-dgoly\n",
      "Processing article: https://egemen.kz/article/9765-maqsat-–-ishki-rynokty-qamtamasyz-etip-eksport-aleuetin-arttyru\n",
      "CSV файл успешно сохранен !\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260239f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae9bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
