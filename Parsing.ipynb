{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76029d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90b288f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re as re\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd00fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the URL for articles in Egemen.kz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "972cacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(base_url,search_url):\n",
    "    response = requests.get(search_url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Парсинг HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    divs = soup.find_all('div', class_='clearfix news-t flexBlock')\n",
    "\n",
    "    # Extract URLs from <a> tags inside those divs\n",
    "    urls = [div.a['href'] for div in divs if div.a]\n",
    "    full_urls = [base_url.rstrip('/') + div.a['href'] for div in divs if div.a]\n",
    "\n",
    "    return full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3c73d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the function specifically for the parsing Egemen.kz web-site content and saved as a text file\n",
    "# Only works for one URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d161b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_contents(article_url,storage_dir):\n",
    "    \n",
    "    # Загрузка HTML страницы\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Парсинг HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Title extract\n",
    "    title = soup.find('h1')\n",
    "    title = title.get_text(strip=True)\n",
    "    \n",
    "    # Author extract\n",
    "\n",
    "    author = soup.find('div', class_='name-auth').text\n",
    "    print(f\"Author:{author}\")\n",
    "    \n",
    "    # Data extract \n",
    "    \n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "\n",
    "    # Extract the 'content' attribute\n",
    "    if date_tag and date_tag.has_attr('content'):\n",
    "        date_published = date_tag['content']\n",
    "    else:\n",
    "        print(\"Date Published not found!\")\n",
    "\n",
    "    date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "    print(date_published_dt)\n",
    "    \n",
    "    # Content extract\n",
    "    article_body = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "\n",
    "    # Extract and clean all text from the div\n",
    "    if article_body:\n",
    "        article_text = article_body.get_text(separator=\"\\n\", strip=True)\n",
    "    else:\n",
    "        print(\"Article body not found!\")\n",
    "    \n",
    "    # Сохранение текста в файл\n",
    "\n",
    "    valid_title = re.sub(r'[\\\\/:\"*?<>|]+', '', title)  # Remove invalid characters\n",
    "    filename = os.path.join(storage_dir, f\"{valid_title}.txt\")\n",
    "\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{title}\\n\\n\")\n",
    "        file.write(article_text)\n",
    "\n",
    "    print(\"Статья успешно сохранена!\")\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06280aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Key word input\n",
    "    key_word_input = input(\"Введите ключевое слово: \\n\")\n",
    "    \n",
    "    # Creating url with key-word\n",
    "    base_url_search = \"https://egemen.kz\"                         #base url can be changed by other URL\n",
    "    key_word_url = base_url_search + \"/search?q=\" + key_word_input \n",
    "\n",
    "    print(f\"Ссылка по вашему запросу: \\n{key_word_url}\")\n",
    "    \n",
    "    # Creating directory with the name of the input \n",
    "    storage_dir = os.path.join(os.getcwd(), key_word_input)\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(key_word_url) \n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parsing HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Finding articles number, included in web-site\n",
    "    article_founded = soup.find('small').text\n",
    "\n",
    "    print(article_founded)\n",
    "    \n",
    "    # Conversion to int from list\n",
    "    num_article = re.findall(r'\\d+', article_founded)\n",
    "    num = int(num_article[0]) \n",
    "    \n",
    "    # Each web-page in site only contains 5 articels, it can be also changed \n",
    "    articles_per_page = 5\n",
    "    pages = math.ceil(num / articles_per_page) \n",
    "\n",
    "    print(f\"Чило страниц:{pages}\\n\")\n",
    "    \n",
    "    urls_list = []  # Initialize the list\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        full_url = key_word_url + \"&page=\" + str(page)\n",
    "        print(f\"Processing page: {page} {full_url}\\n\")\n",
    "    \n",
    "        # Call the function and get the extracted URLs\n",
    "        article_urls = extract_urls(base_url_search, full_url)\n",
    "    \n",
    "        for article_url in article_urls: \n",
    "            print(f\"{article_url}\\n\")    \n",
    "    \n",
    "        # Append URLs to the list\n",
    "        urls_list.extend(article_urls) \n",
    "    \n",
    "        # Skip if no URLs are found\n",
    "        if not article_urls:  \n",
    "            print(f\"Skipping page {page} due to no articles found.\\n\")\n",
    "        continue\n",
    "\n",
    "    # The final list of URLs\n",
    "    print(\"\\nAll Extracted URLs:\\n\")\n",
    "\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "    \n",
    "    print(f\"\\nCount:{len(urls_list)}\\n\")\n",
    "    print(\"Starting saving all the extracted articles to the text\")\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "        article_contents(url,storage_dir)\n",
    "        \n",
    "    # CSV file creation\n",
    "    csv_file = os.path.join(storage_dir, f\"{key_word_input}.csv\")\n",
    "    \n",
    "    # CSV headers\n",
    "    headers = [\"Title\", \"Date Published\", \"Author\", \"URL\"]\n",
    "    \n",
    "\n",
    "        \n",
    "    # Writing to CSV\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for url in urls_list:\n",
    "            print(f\"Processing article: {url}\")\n",
    "            article_details = extract_article_details(url)\n",
    "            writer.writerow(article_details)\n",
    "    print(\"CSV файл успешно сохранен !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ceadcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите ключевое слово: \n",
      "президент путин\n",
      "Ссылка по вашему запросу: \n",
      "https://egemen.kz/search?q=президент путин\n",
      "0 материал табылды\n",
      "Чило страниц:0\n",
      "\n",
      "\n",
      "All Extracted URLs:\n",
      "\n",
      "\n",
      "Count:0\n",
      "\n",
      "Starting saving all the extracted articles to the text\n",
      "CSV файл успешно сохранен !\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1675c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname,_,filenames in os.walk(\"/home/alikhan/Desktop/Data/Parsing\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a11177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Published: 2024-12-18 16:09:00\n",
      "\n",
      "Дана МЫРЗАҚАДІР\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "\n",
    "# Extract the 'content' attribute\n",
    "if date_tag and date_tag.has_attr('content'):\n",
    "    date_published = date_tag['content']\n",
    "    print(\"Date Published:\", date_published)\n",
    "else:\n",
    "    print(\"Date Published not found!\")\n",
    "\n",
    "date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Extracting author to save into the csv\n",
    "\n",
    "author = soup.find('div', class_='name-auth').text\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52e0092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a keyword: \n",
      "Донор\n",
      "Processing page 1: https://egemen.kz/search?q=Донор&page=1\n",
      "Processing article: https://egemen.kz/article/377917-mayittik-donorlyqty-damytugha-ne-kedergi\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'date_published', 'url', 'title', 'author'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll articles saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 87\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing article: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m             article_details \u001b[38;5;241m=\u001b[39m extract_article_details(url)\n\u001b[0;32m---> 87\u001b[0m             \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_details\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll articles saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.12/csv.py:164\u001b[0m, in \u001b[0;36mDictWriter.writerow\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.12/csv.py:159\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    157\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[0;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'date_published', 'url', 'title', 'author'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "import re as regex\n",
    "\n",
    "def extract_article_details(article_url):\n",
    "    \"\"\"Extract title, date published, author, and URL.\"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extract author\n",
    "    author_tag = soup.find('div', class_='name-auth')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"No Author\"\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date_published\": date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\") if date_published_dt else \"No Date\",\n",
    "        \"author\": author,\n",
    "        \"url\": article_url\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Key word input\n",
    "    key_word_input = input(\"Enter a keyword: \\n\").strip()\n",
    "    \n",
    "    # Define storage directory\n",
    "    storage_dir = os.path.join(os.getcwd(), key_word_input)\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "    \n",
    "    # CSV file creation\n",
    "    csv_file = os.path.join(storage_dir, f\"{key_word_input}.csv\")\n",
    "    \n",
    "    # CSV headers\n",
    "    headers = [\"Title\", \"Date Published\", \"Author\", \"URL\"]\n",
    "    \n",
    "    # Writing to CSV\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Base search URL\n",
    "        base_url_search = \"https://egemen.kz\"\n",
    "        key_word_url = base_url_search + \"/search?q=\" + key_word_input\n",
    "        \n",
    "        # Fetch search results\n",
    "        response = requests.get(key_word_url)\n",
    "        soup = BS(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract number of articles\n",
    "        article_founded_tag = soup.find('small')\n",
    "        article_founded = article_founded_tag.text if article_founded_tag else \"0 articles found\"\n",
    "        num_article = regex.findall(r'\\d+', article_founded)\n",
    "        num = int(num_article[0]) if num_article else 0\n",
    "        \n",
    "        # Pages logic\n",
    "        articles_per_page = 5\n",
    "        pages = (num // articles_per_page) + 1\n",
    "        \n",
    "        # Extract articles\n",
    "        for page in range(1, pages + 1):\n",
    "            full_url = key_word_url + \"&page=\" + str(page)\n",
    "            print(f\"Processing page {page}: {full_url}\")\n",
    "            response = requests.get(full_url)\n",
    "            soup = BS(response.text, \"html.parser\")\n",
    "            \n",
    "            # Extract URLs\n",
    "            divs = soup.find_all('div', class_='clearfix news-t flexBlock')\n",
    "            article_urls = [base_url_search.rstrip('/') + div.a['href'] for div in divs if div.a]\n",
    "            \n",
    "            for url in article_urls:\n",
    "                print(f\"Processing article: {url}\")\n",
    "                article_details = extract_article_details(url)\n",
    "                writer.writerow(article_details)\n",
    "    \n",
    "    print(f\"All articles saved to: {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d658827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_details(article_url):\n",
    "    \"\"\"Extract title, date published, author, and URL.\"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_published = date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extract author\n",
    "    author_tag = soup.find('div', class_='name-auth')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"No Author\"\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fc0f674",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'storage_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CSV file creation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mstorage_dir\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mАнализ.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# CSV headers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate Published\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'storage_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# CSV file creation\n",
    "csv_file = os.path.join(storage_dir, f\"Анализ.csv\")\n",
    "    \n",
    "# CSV headers\n",
    "headers = [\"Title\", \"Date Published\", \"Author\", \"URL\"]\n",
    "    \n",
    "\n",
    "\n",
    "# Writing to CSV\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for url in urls_list:\n",
    "        print(f\"Processing article: {url}\")\n",
    "        article_details = extract_article_details(url)\n",
    "        writer.writerow(article_details)  # Pass the entire dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2056c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
