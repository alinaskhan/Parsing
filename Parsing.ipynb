{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb964bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3868c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re as re\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "adb52579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting the URL for articles in Egemen.kz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e100525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(base_url,search_url):\n",
    "    response = requests.get(search_url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Парсинг HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    divs = soup.find_all('div', class_='clearfix news-t flexBlock')\n",
    "\n",
    "    # Extract URLs from <a> tags inside those divs\n",
    "    urls = [div.a['href'] for div in divs if div.a]\n",
    "    full_urls = [base_url.rstrip('/') + div.a['href'] for div in divs if div.a]\n",
    "\n",
    "    return full_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b845469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the function specifically for the parsing Egemen.kz web-site content and saved as a text file\n",
    "# Only works for one URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b2af6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_contents(article_url,storage_dir):\n",
    "    \n",
    "    # Загрузка HTML страницы\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Парсинг HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Title extract\n",
    "    title = soup.find('h1')\n",
    "    title = title.get_text(strip=True)\n",
    "    \n",
    "    # Author extract\n",
    "\n",
    "    author = soup.find('div', class_='name-auth').text\n",
    "    print(f\"Author:{author}\")\n",
    "    \n",
    "    # Data extract \n",
    "    \n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "\n",
    "    # Extract the 'content' attribute\n",
    "    if date_tag and date_tag.has_attr('content'):\n",
    "        date_published = date_tag['content']\n",
    "    else:\n",
    "        print(\"Date Published not found!\")\n",
    "\n",
    "    date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "    print(date_published_dt)\n",
    "    \n",
    "    # Content extract\n",
    "    article_body = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "\n",
    "    # Extract and clean all text from the div\n",
    "    if article_body:\n",
    "        article_text = article_body.get_text(separator=\"\\n\", strip=True)\n",
    "    else:\n",
    "        print(\"Article body not found!\")\n",
    "    \n",
    "    # Сохранение текста в файл\n",
    "\n",
    "    valid_title = re.sub(r'[\\\\/:\"*?<>|]+', '', title)  # Remove invalid characters\n",
    "    filename = os.path.join(storage_dir, f\"{valid_title}.txt\")\n",
    "\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"{title}\\n\\n\")\n",
    "        file.write(article_text)\n",
    "\n",
    "    print(\"Статья успешно сохранена!\")\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d107d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Key word input\n",
    "    key_word_input = input(\"Введите ключевое слово: \\n\")\n",
    "    \n",
    "    # Creating url with key-word\n",
    "    base_url_search = \"https://egemen.kz\"                         #base url can be changed by other URL\n",
    "    key_word_url = base_url_search + \"/search?q=\" + key_word_input \n",
    "\n",
    "    print(f\"Ссылка по вашему запросу: \\n{key_word_url}\")\n",
    "    \n",
    "    # Creating directory with the name of the input \n",
    "    storage_dir = os.path.join(os.getcwd(), key_word_input)\n",
    "    os.makedirs(storage_dir, exist_ok=True)\n",
    "\n",
    "    response = requests.get(key_word_url) \n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parsing HTML\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Finding articles number, included in web-site\n",
    "    article_founded = soup.find('small').text\n",
    "\n",
    "    print(article_founded)\n",
    "    \n",
    "    # Conversion to int from list\n",
    "    num_article = re.findall(r'\\d+', article_founded)\n",
    "    num = int(num_article[0]) \n",
    "    \n",
    "    # Each web-page in site only contains 5 articels, it can be also changed \n",
    "    articles_per_page = 5\n",
    "    pages = math.ceil(num / articles_per_page) \n",
    "\n",
    "    print(f\"Чило страниц:{pages}\\n\")\n",
    "    \n",
    "    urls_list = []  # Initialize the list\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        full_url = key_word_url + \"&page=\" + str(page)\n",
    "        print(f\"Processing page: {page} {full_url}\\n\")\n",
    "    \n",
    "        # Call the function and get the extracted URLs\n",
    "        article_urls = extract_urls(base_url_search, full_url)\n",
    "    \n",
    "        for article_url in article_urls: \n",
    "            print(f\"{article_url}\\n\")    \n",
    "    \n",
    "        # Append URLs to the list\n",
    "        urls_list.extend(article_urls) \n",
    "    \n",
    "        # Skip if no URLs are found\n",
    "        if not article_urls:  \n",
    "            print(f\"Skipping page {page} due to no articles found.\\n\")\n",
    "        continue\n",
    "\n",
    "    # The final list of URLs\n",
    "    print(\"\\nAll Extracted URLs:\\n\")\n",
    "\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "    \n",
    "    print(f\"\\nCount:{len(urls_list)}\\n\")\n",
    "    print(\"Starting saving all the extracted articles to the text\")\n",
    "    for url in urls_list:\n",
    "        print(url)\n",
    "        article_contents(url,storage_dir)\n",
    "        \n",
    "    # CSV file creation\n",
    "    csv_file = os.path.join(storage_dir, f\"{key_word_input}.csv\")\n",
    "    \n",
    "    # CSV headers\n",
    "    headers = [\"Title\", \"Date Published\", \"Author\", \"URL\"]\n",
    "    \n",
    "\n",
    "        \n",
    "    # Writing to CSV\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "    \n",
    "        for url in urls_list:\n",
    "            print(f\"Processing article: {url}\")\n",
    "            article_details = extract_article_details(url)\n",
    "            writer.writerow(article_details)\n",
    "    print(\"CSV файл успешно сохранен !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ea829e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите ключевое слово: \n",
      "президент путин\n",
      "Ссылка по вашему запросу: \n",
      "https://egemen.kz/search?q=президент путин\n",
      "0 материал табылды\n",
      "Чило страниц:0\n",
      "\n",
      "\n",
      "All Extracted URLs:\n",
      "\n",
      "\n",
      "Count:0\n",
      "\n",
      "Starting saving all the extracted articles to the text\n",
      "CSV файл успешно сохранен !\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0124060",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname,_,filenames in os.walk(\"/home/alikhan/Desktop/Data/Parsing\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7c3eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_details(article_url):\n",
    "    \"\"\"Extract title, date published, author, and URL.\"\"\"\n",
    "    response = requests.get(article_url)\n",
    "    html_content = response.text\n",
    "    soup = BS(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "    \n",
    "    # Extract date published\n",
    "    date_tag = soup.find('meta', itemprop=\"datePublished\")\n",
    "    date_published = date_tag['content'] if date_tag and date_tag.has_attr('content') else \"No Date\"\n",
    "    date_published_dt = None\n",
    "    if date_published != \"No Date\":\n",
    "        date_published_dt = datetime.strptime(date_published, \"%Y-%m-%d %H:%M:%S\")\n",
    "        date_published = date_published_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extract author\n",
    "    author_tag = soup.find('div', class_='name-auth')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"No Author\"\n",
    "    \n",
    "    return {\n",
    "        \"Title\": title,\n",
    "        \"Date Published\": date_published,\n",
    "        \"Author\": author,\n",
    "        \"URL\": article_url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720af23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
